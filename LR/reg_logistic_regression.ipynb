{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration number: 0, loss: 0.5287843803108359\n",
      "Current iteration number: 100, loss: 0.13857868630382153\n",
      "Current iteration number: 200, loss: 0.6292596056283293\n",
      "Current iteration number: 300, loss: 0.26031581800597725\n",
      "Current iteration number: 400, loss: 0.30973012334098293\n",
      "Final loss after training: 1.046137076915924\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# 基础函数\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def calculate_loss(y, tx, w, lambda_):\n",
    "    \"\"\"\n",
    "    Compute the regularized loss for logistic regression.\n",
    "    \"\"\"\n",
    "    pred = sigmoid(np.dot(tx, w)).reshape(-1, 1)  # Ensure pred is a column vector\n",
    "    regularization_term = (lambda_ / 2) * np.sum(w**2)\n",
    "    loss = -np.mean(y * np.log(pred + 1e-15) + (1 - y) * np.log(1 - pred + 1e-15)) + regularization_term\n",
    "    return loss\n",
    "\n",
    "def calculate_gradient(y, tx, w, lambda_):\n",
    "    \"\"\"\n",
    "    Compute the regularized gradient for logistic regression.\n",
    "    \"\"\"\n",
    "    pred = sigmoid(np.dot(tx, w)).reshape(-1, 1)\n",
    "    if y.shape != pred.shape:\n",
    "        y = y.reshape(-1, 1)  # Transform y to a column vector\n",
    "    gradient = (np.dot(tx.T, (pred - y)) / len(y)) + (lambda_ * w).reshape(-1, 1) / len(tx)\n",
    "    return gradient.flatten()  # Ensure gradient has the same shape as w\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma, lambda_, batch_size=64):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression using mini-batch gradient descent.\n",
    "    \"\"\"\n",
    "    w = initial_w\n",
    "    n_samples = len(y)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        # Shuffle the data\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        tx = tx[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "        # Mini-batch gradient descent\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_tx = tx[start:end]\n",
    "            batch_y = y[start:end]\n",
    "            \n",
    "            # Compute loss and gradient for the mini-batch\n",
    "            loss = calculate_loss(batch_y, batch_tx, w, lambda_)\n",
    "            gradient = calculate_gradient(batch_y, batch_tx, w, lambda_)\n",
    "            \n",
    "            # Update weights\n",
    "            w = w - gamma * gradient\n",
    "        \n",
    "        # Optionally print progress\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Current iteration number: {i}, loss: {loss}\")\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "def predict(tx, w):\n",
    "    \"\"\"\n",
    "    Predict the class labels using the logistic regression model.\n",
    "    \"\"\"\n",
    "    y_pred = sigmoid(np.dot(tx, w))\n",
    "    y_pred[y_pred > 0.3] = 1\n",
    "    y_pred[y_pred <= 0.3] = 0\n",
    "    return y_pred\n",
    "\n",
    "# 加载数据\n",
    "x_train_pre = np.loadtxt(\"x_train_encoded.csv\", delimiter=\",\", skiprows=1)\n",
    "y_train = np.loadtxt(\"y_train.csv\", delimiter=\",\", skiprows=1)\n",
    "\n",
    "# 提取标签\n",
    "y_train_pre = y_train[:, 1]\n",
    "\n",
    "x_test_pre = np.loadtxt(\"x_test_encoded.csv\", delimiter=\",\", skiprows=1)\n",
    "\n",
    "# 修改标签，将 -1 改为 0\n",
    "y_train_pre[y_train_pre == -1] = 0\n",
    "y_train_pre = y_train_pre.astype(int)\n",
    "\n",
    "# 初始化参数\n",
    "initial_w = np.zeros(x_train_pre.shape[1])  # 权重初始化为零向量\n",
    "max_iters = 500  # 最大迭代次数\n",
    "gamma = 0.01  # 学习率\n",
    "lambda_ = 0.1  # 正则化强度\n",
    "batch_size = 64  # 小批量的大小\n",
    "\n",
    "# 训练正则化逻辑回归模型\n",
    "w, loss = logistic_regression(y_train_pre, x_train_pre, initial_w, max_iters, gamma, lambda_, batch_size)\n",
    "print(f\"Final loss after training: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred 中有 46116 个 NaN\n"
     ]
    }
   ],
   "source": [
    "# 加载sample-submission中的Id列\n",
    "sample_submission = np.loadtxt('sample-submission.csv', delimiter=',', skiprows=1, usecols=0, dtype=int)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred = predict(x_test_pre, w)\n",
    "\n",
    "#y_pred=np.where(y_pred == 0, -1, y_pred)\n",
    "\n",
    "# 查找 y_pred 中 NaN 的个数\n",
    "num_nans = np.sum(np.isnan(y_pred))\n",
    "print(f\"y_pred 中有 {num_nans} 个 NaN\")\n",
    "\n",
    "# 将 y_pred 中的 NaN 替换为 -1\n",
    "y_pred_test = np.nan_to_num(y_pred, nan=-1)\n",
    "# save as .csv\n",
    "header = \"Id,Prediction\"\n",
    "results = np.hstack((sample_submission.reshape(-1, 1), y_pred_test.reshape(-1, 1)))\n",
    "\n",
    "np.savetxt('C:/Users/y/Documents/ml_exercise/ML_project_1/logistic_regression_with_l2.csv', \n",
    "           results, delimiter=',', header=header, comments='', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test on the 10% of x_train_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 424)\n",
      "(32813, 424)\n",
      "(32813,)\n",
      "[0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 10% of the data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "num_samples = x_train_pre.shape[0]\n",
    "sample_size = int(num_samples * 0.1)\n",
    "indices = np.random.choice(num_samples, sample_size, replace=False)\n",
    "\n",
    "# Extract the sampled training data and labels\n",
    "x_train_sample = x_train_pre[indices]\n",
    "y_train_sample = y_train_pre[indices]\n",
    "\n",
    "print(x_train_pre.shape)\n",
    "print(x_train_sample.shape)\n",
    "print(y_train_sample.shape)\n",
    "print(y_train_sample[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4020530367835757 0.893487337335812\n"
     ]
    }
   ],
   "source": [
    "from basic_functions import *\n",
    "# The prediction results of logistic regression\n",
    "y_pred_sample = predict(x_train_sample, w)\n",
    "f1_score_logiregression=calculate_f1_score(y_pred_sample,y_train_sample)\n",
    "accuracy_logiregression=calculate_accuracy(y_pred_sample,y_train_sample)\n",
    "print(f1_score_logiregression,accuracy_logiregression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When lambda=0.1, threshold=0.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
