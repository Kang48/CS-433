import numpy as np

def lasso_preprocess(X, y, lambda_reg=0.1, learning_rate=0.001, num_iters=1000, batch_size=32):
    """
    Apply Lasso (L1 regularization) for feature selection.
    
    Args:
        X (np.ndarray): The input data matrix (n_samples, n_features).
        y (np.ndarray): The target values (n_samples, ).
        lambda_reg (float): The regularization parameter.
        learning_rate (float): The learning rate for gradient descent.
        num_iters (int): Number of iterations for gradient descent.
        batch_size (int): Size of each mini-batch.
        
    Returns:
        np.ndarray: The optimized weights with sparsity due to L1 regularization.
    """
    # Initialize weights
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)

    # Helper function to get mini-batches
    def batch_iterator(X, y, batch_size):
        indices = np.random.permutation(len(y))
        for start in range(0, len(y), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            yield X[batch_indices], y[batch_indices]
    
    # Gradient Descent with L1 regularization
    for i in range(num_iters):
        for X_batch, y_batch in batch_iterator(X, y, batch_size):
            # Predict
            y_pred = X_batch.dot(weights)
            # Compute gradient with L1 penalty
            gradient = (-2 / batch_size) * X_batch.T.dot(y_batch - y_pred) + lambda_reg * np.sign(weights)
            # Update weights
            weights -= learning_rate * gradient

    return weights

# load data
x_train_pre = np.loadtxt("Data\\x_train_pre.csv", delimiter=",", skiprows=1)
y_train = np.loadtxt("Data\\y_train.csv", delimiter=",", skiprows=1)

#提取第二列到最后一列
y_train_pre = y_train[:, 1:]  # 这将返回所有行和从第二列开始的所有列
x_test_pre = np.loadtxt("Data\\x_test_pre.csv", delimiter=",", skiprows=1)

# change the label 
# y_train_pre[y_train_pre == -1] = 0


weights = lasso_preprocess(x_train_pre, y_train_pre, lambda_reg=0.1, learning_rate=0.001, num_iters=1000, batch_size=32)

